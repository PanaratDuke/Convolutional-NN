{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to Visualize the model\n",
    "# %matplotlib inline\n",
    "from IPython.display import Image, SVG\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config OS ENV to allow GPU mememory be scaled for using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction from existing custom made test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image as image_processing\n",
    "from IPython.display import Image as img_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model = load_model(\"./Trained_Model/final_model.h5\")\n",
    "\n",
    "# recompile the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some colors for print-function\n",
    "red = '\\033[31m'\n",
    "bolt = '\\033[1m'\n",
    "blu = '\\033[34m'\n",
    "prp = '\\033[35m'\n",
    "nc = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_predict (img_path):\n",
    "   \n",
    "    img_name = img_path.split(\"/\")[-1]\n",
    "    print(f'{blu}Processing Image :: {prp}{img_name}{nc}')\n",
    "    \n",
    "    \n",
    "    # use keras built in img processing to read input image\n",
    "    image_size = (28, 28)\n",
    "    img = image_processing.load_img(img_path,\n",
    "                                    target_size=image_size,\n",
    "                                    color_mode=\"grayscale\")\n",
    "    \n",
    "    # convert raw img to np arrays\n",
    "    image = img_to_array(img)\n",
    "    \n",
    "    # con2v takes an array of 4 parameters, need to expand to add\n",
    "    # additional dimension to the image\n",
    "    image /= 255\n",
    "    imported_img = img_show(img_path, img_path, width=20, height=20, embed=True)\n",
    "    display(imported_img)\n",
    "    \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Invert the pixel values to match the original data\n",
    "    # when Keras load the picture, the rgb values were inverted\n",
    "    image = 1 - image\n",
    "    \n",
    "    prediction = np.argmax(model.predict(image), axis=-1)\n",
    "    return print(f\"This is number :: {red}{bolt}{prediction[0]}{nc}\\n{('=')*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a list of all picture path inside test image folder\n",
    "base_path = \"./images/test-img/\"\n",
    "img_list = [base_path + img for img in listdir(base_path) if isfile(join(base_path, img))]\n",
    "\n",
    "# looping thru each img and make prediction\n",
    "st_time = time()\n",
    "for ea_iter in tf.range(len(img_list)):\n",
    "    ea_iter = tf.cast(ea_iter, tf.int64)\n",
    "    img_predict(img_list[ea_iter])\n",
    "total_time = time() - st_time \n",
    "print (f'Total Prediction Run Time :: {round(total_time, 3)} seconds || {len(img_list)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Henry Le, 07/17/2020 :: \n",
    "\n",
    "Upon re-training the model, running the handwritten created by MS OneNote Functions, and screen snipping tool, I observed the following:\n",
    "\n",
    "1. Re-training model has potential of changing the prediction, i.e. first model predicted right, then next rerun predict number wrong.     \n",
    "\n",
    "\n",
    "2. The original snipped file would be predicted more accurately if the image has width == height. This is due to the fact that the MNIST dataset has width == height, thus the trained model weight matrices retain the same shape. When I purposely create a test handwritten with width !== height, although it is the exact same number, it is still make the model predict the number wrong. This I think due to when Keras try to resize the image, the original image was distorted. It is same as trying to resize the image without retaining the aspect ratio, such as if height >> width, the height dimension will be compressed more significant than the width. Thus the image is distorted, and make the prediction wrong.\n",
    "    * As in the case showing above of hand8-not-sq.JPG (Keras will resize to width == height) and hand8-distorted.jpg (where I pre-resize before loading into the model) :: model in both cases predict this as number 9 instead of 8.  \n",
    "\n",
    "\n",
    "3. Thinner stroke line tends to make the prediction wrong. Increase the stroke line would help the model predict more accurately. This could be because as thicker stroke, the features of the image is hightlighted better so that during the reshape from large pixel of for instance 100px x 100px down to 28px x 28px, the black stroke is still thick enough for the model to recognize it.  \n",
    "    * As in the case showing above of 8.jpg and 8-thicker.jpg. The thinner 8 made the model predict inaccurately. \n",
    "    \n",
    "\n",
    "4. Number 9 is challenging to recognize in with this particular train model, which predicts more 4 and 7 more often than the correct number 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list of all picture path inside test image folder\n",
    "base_path = \"./images/test-img/num7-variant/\"\n",
    "img_list = [base_path + img for img in listdir(base_path) if isfile(join(base_path, img))]\n",
    "\n",
    "# looping thru each img and make prediction\n",
    "for ea_iter in tf.range(len(img_list)):\n",
    "    ea_iter = tf.cast(ea_iter, tf.int64)\n",
    "    img_predict(img_list[ea_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
